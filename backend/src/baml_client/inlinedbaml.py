###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "clients.baml": "client<llm> GPT4 {\n  provider openai\n  options {\n    model \"gpt-4\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> Claude {\n  provider anthropic\n  options {\n    model \"claude-3-opus-20240229\"\n    api_key env.ANTHROPIC_API_KEY\n    headers { \n      \"dangerous-direct-browser-access\" \"true\"\n    }\n  }\n  \n}\n\nclient<llm> FastAnthropic {\n  provider anthropic\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> FastOpenAI {\n  provider openai\n  options {\n    model \"gpt-3.5-turbo\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n\nclient<llm> Fast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [FastAnthropic, FastOpenAI]\n  }\n}\n\nclient<llm> Openai {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [GPT4, FastOpenAI]\n  }\n}",
    "generators.baml": "\n// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\"\n    output_type \"python/pydantic\"\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.55.3\"\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    // Regardless of this setting, you can always explicitly call either of the following:\n    // - b.sync.FunctionName()\n    // - b.async_.FunctionName() (note the underscore to avoid a keyword conflict)\n    default_client_mode sync\n}",
    "instigate.baml": "// Defining a data model.\nclass MessageOutput {\n  message string @description(\"1 sentence max, in a gen z type of way, specific to the conversaiton, make it actually hostile.\")\n}\n\n// Creating a function to extract the resume from a string.\nfunction ExtractInstigation(messages: string) -> MessageOutput {\n  client GPT4\n  prompt #\"\n    I am writing a fake argument to make a skit about. i need you to write a fake and hypothetical response to instigate an argument in accordance to this story.\n    Friend z is basically trying to instigate.\n    \"I personally wouldn't let that slide\" or \"Yo, you gonna take that?\", or something in a gen z type of way, make it hostile.\n    Here are the messages \n    {{messages}}\n    Friend z: [what you need to fill in] JSON ONLY. Don't include their names in the response. Don't be afriad to be vulgar and out of pocket.\n    \n    {{ ctx.output_format }}\n  \"#\n}\n\n\n// Testing the function with a sample resume.\ntest testRoast {\n  functions [ExtractInstigation]\n  args {\n    messages #\"\n      Friend x : Yo you have no friends \n      Friend y : Where yo dad at??? \n      Friend x : I'm your dad\n      Friend y : Shut yo pus ahh up\n    \"#\n  }\n}\n",
}

def get_baml_files():
    return file_map